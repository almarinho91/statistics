{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "Implementation of resampling techniques for model validation and uncertainty estimation.\n",
    "\n",
    "## Contents\n",
    "1. Train/Test Split Validation\n",
    "2. Leave-One-Out Cross-Validation (LOOCV)\n",
    "3. K-Fold Cross-Validation\n",
    "4. Bootstrap for Parameter Estimation\n",
    "5. Bootstrap for Model Coefficient Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.utils import resample\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Auto Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Auto dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "column_names = ['mpg', 'cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'year', 'origin', 'name']\n",
    "Auto = pd.read_csv(url, sep='\\s+', names=column_names, na_values='?')\n",
    "Auto = Auto.dropna()\n",
    "\n",
    "# Convert horsepower to numeric\n",
    "Auto['horsepower'] = pd.to_numeric(Auto['horsepower'])\n",
    "\n",
    "print(f\"Dataset shape: {Auto.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "Auto.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Simple Train/Test Split Validation\n",
    "\n",
    "Split data into training (50%) and testing (50%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "np.random.seed(1)\n",
    "\n",
    "# Prepare data\n",
    "X = Auto[['horsepower']]\n",
    "y = Auto['mpg']\n",
    "\n",
    "# Split: 50% train, 50% test (same as R: train=sample(392,196))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model (Degree 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit linear model\n",
    "model_linear = LinearRegression()\n",
    "model_linear.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = model_linear.predict(X_test)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse_linear = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Linear Model (degree 1)\")\n",
    "print(f\"Test MSE: {mse_linear:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Models (Degree 2 and 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial degree 2\n",
    "poly2 = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_train_poly2 = poly2.fit_transform(X_train)\n",
    "X_test_poly2 = poly2.transform(X_test)\n",
    "\n",
    "model_poly2 = LinearRegression()\n",
    "model_poly2.fit(X_train_poly2, y_train)\n",
    "y_pred_poly2 = model_poly2.predict(X_test_poly2)\n",
    "mse_poly2 = mean_squared_error(y_test, y_pred_poly2)\n",
    "\n",
    "print(f\"Polynomial Model (degree 2)\")\n",
    "print(f\"Test MSE: {mse_poly2:.4f}\")\n",
    "\n",
    "# Polynomial degree 3\n",
    "poly3 = PolynomialFeatures(degree=3, include_bias=False)\n",
    "X_train_poly3 = poly3.fit_transform(X_train)\n",
    "X_test_poly3 = poly3.transform(X_test)\n",
    "\n",
    "model_poly3 = LinearRegression()\n",
    "model_poly3.fit(X_train_poly3, y_train)\n",
    "y_pred_poly3 = model_poly3.predict(X_test_poly3)\n",
    "mse_poly3 = mean_squared_error(y_test, y_pred_poly3)\n",
    "\n",
    "print(f\"\\nPolynomial Model (degree 3)\")\n",
    "print(f\"Test MSE: {mse_poly3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "results_train_test = pd.DataFrame({\n",
    "    'Model': ['Linear', 'Polynomial (deg 2)', 'Polynomial (deg 3)'],\n",
    "    'Test MSE': [mse_linear, mse_poly2, mse_poly3]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAIN/TEST SPLIT VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(results_train_test)\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNote: Results depend on the random train/test split!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Different Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different random seeds to show variability\n",
    "seeds = [1, 2, 3, 4, 5]\n",
    "results_by_seed = []\n",
    "\n",
    "for seed in seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=seed)\n",
    "    \n",
    "    # Linear\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    mse_lin = mean_squared_error(y_test, model.predict(X_test))\n",
    "    \n",
    "    # Polynomial degree 2\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    mse_poly = mean_squared_error(y_test, model.predict(X_test_poly))\n",
    "    \n",
    "    results_by_seed.append({'Seed': seed, 'Linear MSE': mse_lin, 'Poly2 MSE': mse_poly})\n",
    "\n",
    "df_seeds = pd.DataFrame(results_by_seed)\n",
    "print(\"\\nTest MSE with Different Random Seeds:\")\n",
    "print(df_seeds)\n",
    "print(f\"\\nLinear MSE: mean={df_seeds['Linear MSE'].mean():.2f}, std={df_seeds['Linear MSE'].std():.2f}\")\n",
    "print(f\"Poly2 MSE: mean={df_seeds['Poly2 MSE'].mean():.2f}, std={df_seeds['Poly2 MSE'].std():.2f}\")\n",
    "print(\"\\n→ Results vary based on train/test split! Need better validation method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Leave-One-Out Cross-Validation (LOOCV)\n",
    "\n",
    "Train on n-1 observations, test on 1 observation. Repeat n times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOOCV for linear model\n",
    "loo = LeaveOneOut()\n",
    "model = LinearRegression()\n",
    "\n",
    "# Perform LOOCV\n",
    "mse_scores = []\n",
    "for train_idx, test_idx in loo.split(X):\n",
    "    X_train_loo, X_test_loo = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train_loo, y_test_loo = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    model.fit(X_train_loo, y_train_loo)\n",
    "    y_pred_loo = model.predict(X_test_loo)\n",
    "    mse_scores.append(mean_squared_error(y_test_loo, y_pred_loo))\n",
    "\n",
    "loocv_mse = np.mean(mse_scores)\n",
    "print(f\"LOOCV MSE (Linear): {loocv_mse:.4f}\")\n",
    "print(f\"Number of iterations: {len(mse_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOOCV for Polynomial Models (Degrees 1-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute LOOCV error for polynomial models of degree 1 to 5\n",
    "loocv_errors = []\n",
    "\n",
    "for degree in range(1, 6):\n",
    "    print(f\"Computing LOOCV for degree {degree}...\", end=' ')\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    mse_scores = []\n",
    "    for train_idx, test_idx in loo.split(X_poly):\n",
    "        X_train_loo = X_poly[train_idx]\n",
    "        X_test_loo = X_poly[test_idx]\n",
    "        y_train_loo = y.iloc[train_idx]\n",
    "        y_test_loo = y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train_loo, y_train_loo)\n",
    "        y_pred_loo = model.predict(X_test_loo)\n",
    "        mse_scores.append(mean_squared_error(y_test_loo, y_pred_loo))\n",
    "    \n",
    "    loocv_mse = np.mean(mse_scores)\n",
    "    loocv_errors.append(loocv_mse)\n",
    "    print(f\"MSE = {loocv_mse:.4f}\")\n",
    "\n",
    "# Display results\n",
    "loocv_results = pd.DataFrame({\n",
    "    'Degree': range(1, 6),\n",
    "    'LOOCV MSE': loocv_errors\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOOCV RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(loocv_results)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LOOCV errors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 6), loocv_errors, marker='o', linewidth=2, markersize=8)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('LOOCV MSE')\n",
    "plt.title('Leave-One-Out Cross-Validation Error')\n",
    "plt.xticks(range(1, 6))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest degree: {loocv_results.loc[loocv_results['LOOCV MSE'].idxmin(), 'Degree']:.0f}\")\n",
    "print(f\"Lowest MSE: {loocv_results['LOOCV MSE'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. K-Fold Cross-Validation\n",
    "\n",
    "Divide data into K folds. Train on K-1 folds, test on 1 fold. Repeat K times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(17)\n",
    "\n",
    "# 10-Fold Cross-Validation for polynomial models\n",
    "kfold_errors = []\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=17)\n",
    "\n",
    "for degree in range(1, 11):\n",
    "    print(f\"Computing 10-Fold CV for degree {degree}...\", end=' ')\n",
    "    \n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    \n",
    "    # Use sklearn's cross_val_score\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # cross_val_score returns negative MSE, so we negate it\n",
    "    scores = cross_val_score(model, X_poly, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "    mse = -scores.mean()\n",
    "    \n",
    "    kfold_errors.append(mse)\n",
    "    print(f\"MSE = {mse:.4f}\")\n",
    "\n",
    "# Display results\n",
    "kfold_results = pd.DataFrame({\n",
    "    'Degree': range(1, 11),\n",
    "    '10-Fold CV MSE': kfold_errors\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"10-FOLD CROSS-VALIDATION RESULTS\")\n",
    "print(\"=\"*50)\n",
    "print(kfold_results)\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare LOOCV vs 10-Fold CV\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.plot(range(1, 6), loocv_errors, marker='o', linewidth=2, markersize=8, label='LOOCV')\n",
    "plt.plot(range(1, 11), kfold_errors, marker='s', linewidth=2, markersize=8, label='10-Fold CV')\n",
    "\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Cross-Validation MSE')\n",
    "plt.title('LOOCV vs 10-Fold Cross-Validation')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest degree (10-Fold): {kfold_results.loc[kfold_results['10-Fold CV MSE'].idxmin(), 'Degree']:.0f}\")\n",
    "print(f\"Lowest MSE: {kfold_results['10-Fold CV MSE'].min():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bootstrap\n",
    "\n",
    "Resample data with replacement to estimate parameter uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap for Alpha (Portfolio Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Portfolio dataset\n",
    "url_portfolio = \"https://raw.githubusercontent.com/selva86/datasets/master/Portfolio.csv\"\n",
    "try:\n",
    "    Portfolio = pd.read_csv(url_portfolio)\n",
    "    print(f\"Portfolio dataset shape: {Portfolio.shape}\")\n",
    "    Portfolio.head()\n",
    "except:\n",
    "    # Create synthetic data if loading fails\n",
    "    np.random.seed(1)\n",
    "    Portfolio = pd.DataFrame({\n",
    "        'X': np.random.randn(100),\n",
    "        'Y': np.random.randn(100)\n",
    "    })\n",
    "    print(\"Using synthetic Portfolio data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha function: estimates optimal allocation between two investments\n",
    "def alpha_fn(data, indices):\n",
    "    \"\"\"\n",
    "    Estimate alpha: optimal fraction to invest in asset X vs Y.\n",
    "    \n",
    "    Formula: alpha = (var(Y) - cov(X,Y)) / (var(X) + var(Y) - 2*cov(X,Y))\n",
    "    \"\"\"\n",
    "    X = data['X'].iloc[indices]\n",
    "    Y = data['Y'].iloc[indices]\n",
    "    \n",
    "    var_X = np.var(X, ddof=1)\n",
    "    var_Y = np.var(Y, ddof=1)\n",
    "    cov_XY = np.cov(X, Y)[0, 1]\n",
    "    \n",
    "    alpha = (var_Y - cov_XY) / (var_X + var_Y - 2 * cov_XY)\n",
    "    return alpha\n",
    "\n",
    "# Estimate alpha on full dataset\n",
    "alpha_full = alpha_fn(Portfolio, range(len(Portfolio)))\n",
    "print(f\"Alpha estimate (full dataset): {alpha_full:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate alpha on a bootstrap sample\n",
    "np.random.seed(1)\n",
    "boot_indices = np.random.choice(len(Portfolio), size=len(Portfolio), replace=True)\n",
    "alpha_boot = alpha_fn(Portfolio, boot_indices)\n",
    "print(f\"Alpha estimate (bootstrap sample): {alpha_boot:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform 1000 Bootstrap Replications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap with 1000 replications\n",
    "np.random.seed(1)\n",
    "n_bootstrap = 1000\n",
    "alpha_estimates = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    boot_indices = np.random.choice(len(Portfolio), size=len(Portfolio), replace=True)\n",
    "    alpha = alpha_fn(Portfolio, boot_indices)\n",
    "    alpha_estimates.append(alpha)\n",
    "\n",
    "alpha_estimates = np.array(alpha_estimates)\n",
    "\n",
    "# Bootstrap statistics\n",
    "alpha_mean = np.mean(alpha_estimates)\n",
    "alpha_std = np.std(alpha_estimates, ddof=1)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"BOOTSTRAP RESULTS FOR ALPHA\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Original estimate: {alpha_full:.4f}\")\n",
    "print(f\"Bootstrap mean: {alpha_mean:.4f}\")\n",
    "print(f\"Bootstrap SE: {alpha_std:.4f}\")\n",
    "print(f\"Number of replications: {n_bootstrap}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(alpha_estimates, bins=30, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(alpha_mean, color='red', linestyle='--', linewidth=2, label=f'Mean = {alpha_mean:.3f}')\n",
    "plt.axvline(alpha_full, color='green', linestyle='--', linewidth=2, label=f'Original = {alpha_full:.3f}')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title(f'Bootstrap Distribution of Alpha (n={n_bootstrap})')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bootstrap for Linear Regression Coefficients\n",
    "\n",
    "Estimate uncertainty in regression coefficients using bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to estimate regression coefficients\n",
    "def boot_fn_regression(data, indices):\n",
    "    \"\"\"\n",
    "    Estimate regression coefficients: mpg ~ horsepower\n",
    "    \"\"\"\n",
    "    data_boot = data.iloc[indices]\n",
    "    X_boot = data_boot[['horsepower']]\n",
    "    y_boot = data_boot['mpg']\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_boot, y_boot)\n",
    "    \n",
    "    return [model.intercept_, model.coef_[0]]\n",
    "\n",
    "# Estimate on full dataset\n",
    "coef_full = boot_fn_regression(Auto, range(len(Auto)))\n",
    "print(f\"Coefficients (full dataset):\")\n",
    "print(f\"Intercept: {coef_full[0]:.4f}\")\n",
    "print(f\"Horsepower: {coef_full[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap samples\n",
    "np.random.seed(1)\n",
    "\n",
    "print(\"\\nBootstrap samples:\")\n",
    "for i in range(3):\n",
    "    boot_indices = np.random.choice(len(Auto), size=len(Auto), replace=True)\n",
    "    coefs = boot_fn_regression(Auto, boot_indices)\n",
    "    print(f\"Sample {i+1}: Intercept={coefs[0]:.4f}, Horsepower={coefs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 1000 bootstrap replications\n",
    "np.random.seed(1)\n",
    "n_bootstrap = 1000\n",
    "intercepts = []\n",
    "slopes = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    boot_indices = np.random.choice(len(Auto), size=len(Auto), replace=True)\n",
    "    coefs = boot_fn_regression(Auto, boot_indices)\n",
    "    intercepts.append(coefs[0])\n",
    "    slopes.append(coefs[1])\n",
    "\n",
    "intercepts = np.array(intercepts)\n",
    "slopes = np.array(slopes)\n",
    "\n",
    "# Bootstrap statistics\n",
    "print(\"=\"*50)\n",
    "print(\"BOOTSTRAP RESULTS FOR REGRESSION COEFFICIENTS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Intercept:\")\n",
    "print(f\"  Original: {coef_full[0]:.4f}\")\n",
    "print(f\"  Bootstrap mean: {np.mean(intercepts):.4f}\")\n",
    "print(f\"  Bootstrap SE: {np.std(intercepts, ddof=1):.4f}\")\n",
    "print(f\"\\nHorsepower coefficient:\")\n",
    "print(f\"  Original: {coef_full[1]:.4f}\")\n",
    "print(f\"  Bootstrap mean: {np.mean(slopes):.4f}\")\n",
    "print(f\"  Bootstrap SE: {np.std(slopes, ddof=1):.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with standard formula SE\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "X_sm = sm.add_constant(Auto[['horsepower']])\n",
    "model_sm = sm.OLS(Auto['mpg'], X_sm).fit()\n",
    "\n",
    "print(\"\\nComparison: Bootstrap vs Standard Formula\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Coefficient':<15} {'Bootstrap SE':<15} {'Formula SE':<15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Intercept':<15} {np.std(intercepts, ddof=1):<15.4f} {model_sm.bse[0]:<15.4f}\")\n",
    "print(f\"{'Horsepower':<15} {np.std(slopes, ddof=1):<15.4f} {model_sm.bse[1]:<15.4f}\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nNote: Bootstrap SE is often more accurate for non-normal errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize bootstrap distributions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Intercept\n",
    "axes[0].hist(intercepts, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0].axvline(np.mean(intercepts), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean = {np.mean(intercepts):.2f}')\n",
    "axes[0].axvline(coef_full[0], color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Original = {coef_full[0]:.2f}')\n",
    "axes[0].set_xlabel('Intercept')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Bootstrap Distribution: Intercept')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Slope\n",
    "axes[1].hist(slopes, bins=30, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "axes[1].axvline(np.mean(slopes), color='red', linestyle='--', linewidth=2, \n",
    "                label=f'Mean = {np.mean(slopes):.4f}')\n",
    "axes[1].axvline(coef_full[1], color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Original = {coef_full[1]:.4f}')\n",
    "axes[1].set_xlabel('Horsepower Coefficient')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Bootstrap Distribution: Slope')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap for Quadratic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap for quadratic model: mpg ~ horsepower + horsepower^2\n",
    "def boot_fn_quadratic(data, indices):\n",
    "    data_boot = data.iloc[indices]\n",
    "    X_boot = data_boot[['horsepower']]\n",
    "    y_boot = data_boot['mpg']\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "    X_poly_boot = poly.fit_transform(X_boot)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_poly_boot, y_boot)\n",
    "    \n",
    "    return [model.intercept_, model.coef_[0], model.coef_[1]]\n",
    "\n",
    "# Full dataset\n",
    "coef_quad_full = boot_fn_quadratic(Auto, range(len(Auto)))\n",
    "print(f\"Quadratic model coefficients (full dataset):\")\n",
    "print(f\"Intercept: {coef_quad_full[0]:.4f}\")\n",
    "print(f\"Horsepower: {coef_quad_full[1]:.4f}\")\n",
    "print(f\"Horsepower²: {coef_quad_full[2]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap for quadratic model\n",
    "np.random.seed(1)\n",
    "n_bootstrap = 1000\n",
    "coef_quad_boot = []\n",
    "\n",
    "for i in range(n_bootstrap):\n",
    "    boot_indices = np.random.choice(len(Auto), size=len(Auto), replace=True)\n",
    "    coefs = boot_fn_quadratic(Auto, boot_indices)\n",
    "    coef_quad_boot.append(coefs)\n",
    "\n",
    "coef_quad_boot = np.array(coef_quad_boot)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"BOOTSTRAP RESULTS: QUADRATIC MODEL\")\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Coefficient':<20} {'Bootstrap SE':<15}\")\n",
    "print(\"-\"*50)\n",
    "print(f\"{'Intercept':<20} {np.std(coef_quad_boot[:, 0], ddof=1):<15.4f}\")\n",
    "print(f\"{'Horsepower':<20} {np.std(coef_quad_boot[:, 1], ddof=1):<15.4f}\")\n",
    "print(f\"{'Horsepower²':<20} {np.std(coef_quad_boot[:, 2], ddof=1):<15.4f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "### **Validation Set Approach**\n",
    "- Simple train/test split\n",
    "- Results depend on random split\n",
    "- Uses only 50% of data for training\n",
    "\n",
    "### **Leave-One-Out Cross-Validation (LOOCV)**\n",
    "- Uses n-1 observations for training\n",
    "- Less bias (more data for training)\n",
    "- Computationally expensive (n iterations)\n",
    "- Lower variance in error estimate\n",
    "\n",
    "### **K-Fold Cross-Validation**\n",
    "- Compromise between validation set and LOOCV\n",
    "- K=10 is common choice\n",
    "- Less computationally expensive than LOOCV\n",
    "- Good bias-variance trade-off\n",
    "\n",
    "### **Bootstrap**\n",
    "- Resample data with replacement\n",
    "- Estimate parameter uncertainty\n",
    "- More accurate SE for non-normal errors\n",
    "- Flexible: works for any statistic\n",
    "\n",
    "### **Key Takeaways:**\n",
    "- Cross-validation helps select model complexity\n",
    "- LOOCV has low bias but high variance\n",
    "- K-Fold CV (K=5 or 10) is a good default\n",
    "- Bootstrap provides robust SE estimates\n",
    "- Always validate on held-out data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}