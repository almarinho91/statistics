{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Dimensionality reduction and exploratory data analysis using PCA.\n",
    "\n",
    "## Contents\n",
    "1. Data Exploration and Standardization\n",
    "2. Computing Principal Components\n",
    "3. Variance Explained\n",
    "4. Biplots\n",
    "5. Interpreting Principal Components\n",
    "6. Choosing Number of Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration: USArrests Dataset\n",
    "\n",
    "Dataset contains statistics (per 100,000 residents) for arrests in 50 US states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load USArrests dataset\n",
    "url = \"https://raw.githubusercontent.com/selva86/datasets/master/USArrests.csv\"\n",
    "USArrests = pd.read_csv(url)\n",
    "\n",
    "# Check if first column is state names (unnamed or 'Unnamed: 0')\n",
    "if USArrests.columns[0] in ['Unnamed: 0', 'State'] or USArrests.iloc[0, 0] == 'Alabama':\n",
    "    USArrests = USArrests.set_index(USArrests.columns[0])\n",
    "    USArrests.index.name = 'State'\n",
    "\n",
    "print(f\"Dataset shape: {USArrests.shape}\")\n",
    "print(f\"\\nStates (first 10):\")\n",
    "print(USArrests.index[:10].tolist())\n",
    "print(f\"\\nVariables:\")\n",
    "print(USArrests.columns.tolist())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "USArrests.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary Statistics:\")\n",
    "print(USArrests.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Mean and Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and variance for each variable\n",
    "means = USArrests.mean()\n",
    "variances = USArrests.var()\n",
    "\n",
    "print(\"Means:\")\n",
    "print(means)\n",
    "print(f\"\\nVariances:\")\n",
    "print(variances)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"IMPORTANT: Variables have very different scales!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Murder: mean = {means['Murder']:.2f}, var = {variances['Murder']:.2f}\")\n",
    "print(f\"UrbanPop: mean = {means['UrbanPop']:.2f}, var = {variances['UrbanPop']:.2f}\")\n",
    "print(f\"\\nWe MUST standardize before PCA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "for idx, col in enumerate(USArrests.columns):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    ax.hist(USArrests[col], bins=15, edgecolor='black', alpha=0.7)\n",
    "    ax.set_xlabel(col, fontsize=12)\n",
    "    ax.set_ylabel('Frequency', fontsize=12)\n",
    "    ax.set_title(f'{col} Distribution', fontsize=12)\n",
    "    ax.axvline(means[col], color='red', linestyle='--', linewidth=2, label=f'Mean = {means[col]:.1f}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis\n",
    "\n",
    "Standardize data and compute principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "USArrests_scaled = scaler.fit_transform(USArrests)\n",
    "\n",
    "# Verify standardization\n",
    "print(\"After standardization:\")\n",
    "print(f\"Means (should be ~0): {USArrests_scaled.mean(axis=0)}\")\n",
    "print(f\"Std devs (should be 1): {USArrests_scaled.std(axis=0, ddof=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "pca = PCA()\n",
    "pca.fit(USArrests_scaled)\n",
    "\n",
    "print(\"PCA Components:\")\n",
    "print(f\"Number of components: {pca.n_components_}\")\n",
    "print(f\"\\nComponent shape: {pca.components_.shape}\")\n",
    "print(f\"(4 components × 4 original features)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Loadings (Rotation Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loadings (rotation matrix)\n",
    "loadings = pd.DataFrame(\n",
    "    pca.components_.T,  # Transpose to match R output\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=USArrests.columns\n",
    ")\n",
    "\n",
    "print(\"Principal Component Loadings:\")\n",
    "print(loadings)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"Interpretation of PC1:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PC1 = {loadings.loc['Murder', 'PC1']:.3f} × Murder\")\n",
    "print(f\"    + {loadings.loc['Assault', 'PC1']:.3f} × Assault\")\n",
    "print(f\"    + {loadings.loc['UrbanPop', 'PC1']:.3f} × UrbanPop\")\n",
    "print(f\"    + {loadings.loc['Rape', 'PC1']:.3f} × Rape\")\n",
    "print(f\"\\nPC1 represents overall crime rate (all crimes have similar weights)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PC1 vs PC2 loadings\n",
    "axes[0].scatter(loadings['PC1'], loadings['PC2'], s=100)\n",
    "for i, var in enumerate(loadings.index):\n",
    "    axes[0].annotate(var, (loadings.loc[var, 'PC1'], loadings.loc[var, 'PC2']),\n",
    "                    fontsize=12, ha='center', va='bottom')\n",
    "    axes[0].arrow(0, 0, loadings.loc[var, 'PC1']*0.9, loadings.loc[var, 'PC2']*0.9,\n",
    "                 head_width=0.05, head_length=0.05, fc='blue', ec='blue', alpha=0.5)\n",
    "axes[0].axhline(0, color='black', linewidth=0.5)\n",
    "axes[0].axvline(0, color='black', linewidth=0.5)\n",
    "axes[0].set_xlabel('PC1 Loading', fontsize=12)\n",
    "axes[0].set_ylabel('PC2 Loading', fontsize=12)\n",
    "axes[0].set_title('Variable Loadings on PC1 and PC2', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap of all loadings\n",
    "sns.heatmap(loadings, annot=True, fmt='.3f', cmap='RdBu_r', center=0,\n",
    "           cbar_kws={'label': 'Loading'}, ax=axes[1])\n",
    "axes[1].set_title('All Principal Component Loadings', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data to PC space\n",
    "scores = pca.transform(USArrests_scaled)\n",
    "scores_df = pd.DataFrame(\n",
    "    scores,\n",
    "    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    index=USArrests.index\n",
    ")\n",
    "\n",
    "print(f\"Principal Component Scores shape: {scores_df.shape}\")\n",
    "print(f\"\\nFirst few states:\")\n",
    "print(scores_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variance Explained\n",
    "\n",
    "How much variance does each PC capture?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation of each PC\n",
    "std_devs = np.sqrt(pca.explained_variance_)\n",
    "\n",
    "print(\"Standard Deviations of Principal Components:\")\n",
    "for i, std in enumerate(std_devs):\n",
    "    print(f\"  PC{i+1}: {std:.4f}\")\n",
    "\n",
    "# Variance explained by each PC\n",
    "var_explained = pca.explained_variance_\n",
    "\n",
    "print(\"\\nVariance Explained by Each PC:\")\n",
    "for i, var in enumerate(var_explained):\n",
    "    print(f\"  PC{i+1}: {var:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of variance explained (PVE)\n",
    "pve = pca.explained_variance_ratio_\n",
    "\n",
    "print(\"Proportion of Variance Explained (PVE):\")\n",
    "for i, ratio in enumerate(pve):\n",
    "    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nTotal variance explained by all PCs: {pve.sum():.4f} (100%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative variance explained\n",
    "cumulative_pve = np.cumsum(pve)\n",
    "\n",
    "print(\"Cumulative Variance Explained:\")\n",
    "for i, cum_var in enumerate(cumulative_pve):\n",
    "    print(f\"  PC1 to PC{i+1}: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"First 2 PCs explain {cumulative_pve[1]*100:.1f}% of variance!\")\n",
    "print(f\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot and cumulative variance plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Scree plot (Proportion of variance explained)\n",
    "axes[0].plot(range(1, len(pve)+1), pve, marker='o', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Proportion of Variance Explained', fontsize=12)\n",
    "axes[0].set_title('Scree Plot', fontsize=14)\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].set_xticks(range(1, len(pve)+1))\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance plot\n",
    "axes[1].plot(range(1, len(cumulative_pve)+1), cumulative_pve, \n",
    "            marker='o', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axhline(0.8, color='red', linestyle='--', linewidth=1, label='80% threshold')\n",
    "axes[1].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Proportion of Variance Explained', fontsize=12)\n",
    "axes[1].set_title('Cumulative Variance Explained', fontsize=14)\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].set_xticks(range(1, len(cumulative_pve)+1))\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Elbow in scree plot suggests using 2-3 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Biplot\n",
    "\n",
    "Visualize both observations (states) and variables in PC space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biplot\n",
    "def biplot(scores, loadings, labels=None, pc1=0, pc2=1):\n",
    "    \"\"\"\n",
    "    Create a biplot showing both observations and variables.\n",
    "    \n",
    "    Parameters:\n",
    "    - scores: Principal component scores (observations)\n",
    "    - loadings: Principal component loadings (variables)\n",
    "    - labels: Labels for observations\n",
    "    - pc1, pc2: Which PCs to plot (0-indexed)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Plot observations (states)\n",
    "    ax.scatter(scores[:, pc1], scores[:, pc2], alpha=0.6, s=50, c='blue')\n",
    "    \n",
    "    # Add state labels\n",
    "    if labels is not None:\n",
    "        for i, label in enumerate(labels):\n",
    "            ax.annotate(label, (scores[i, pc1], scores[i, pc2]),\n",
    "                       fontsize=8, alpha=0.7)\n",
    "    \n",
    "    # Scale factor for arrows\n",
    "    scale = 4\n",
    "    \n",
    "    # Plot variables (arrows)\n",
    "    for i, var in enumerate(loadings.index):\n",
    "        ax.arrow(0, 0, \n",
    "                loadings.iloc[i, pc1] * scale, \n",
    "                loadings.iloc[i, pc2] * scale,\n",
    "                head_width=0.2, head_length=0.2, \n",
    "                fc='red', ec='red', linewidth=2, alpha=0.7)\n",
    "        ax.text(loadings.iloc[i, pc1] * scale * 1.15,\n",
    "               loadings.iloc[i, pc2] * scale * 1.15,\n",
    "               var, fontsize=12, color='red', fontweight='bold')\n",
    "    \n",
    "    ax.axhline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    ax.axvline(0, color='black', linewidth=0.5, linestyle='--')\n",
    "    ax.set_xlabel(f'PC{pc1+1} ({pve[pc1]*100:.1f}% variance)', fontsize=12)\n",
    "    ax.set_ylabel(f'PC{pc2+1} ({pve[pc2]*100:.1f}% variance)', fontsize=12)\n",
    "    ax.set_title('PCA Biplot', fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    return fig, ax\n",
    "\n",
    "# Create biplot\n",
    "fig, ax = biplot(scores, loadings, labels=USArrests.index)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Biplot Interpretation:\")\n",
    "print(\"- Blue points = States\")\n",
    "print(\"- Red arrows = Original variables\")\n",
    "print(\"- States in direction of arrow have high values for that variable\")\n",
    "print(\"- Longer arrow = more variance in that direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sign Ambiguity\n",
    "\n",
    "Principal components are unique up to a sign change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip signs of loadings and scores\n",
    "loadings_flipped = -loadings\n",
    "scores_flipped = -scores\n",
    "\n",
    "# Create biplot with flipped signs\n",
    "fig, ax = biplot(scores_flipped, loadings_flipped, labels=USArrests.index)\n",
    "plt.title('PCA Biplot (signs flipped)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: Sign flip doesn't change interpretation!\")\n",
    "print(\"Relative positions of states and variables remain the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interpreting Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# States with highest/lowest scores on PC1\n",
    "pc1_scores = scores_df['PC1'].sort_values()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PC1 INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPC1 loadings:\")\n",
    "print(loadings['PC1'].sort_values(ascending=False))\n",
    "\n",
    "print(f\"\\nStates with LOWEST PC1 (low crime):\")\n",
    "print(pc1_scores.head(5))\n",
    "\n",
    "print(f\"\\nStates with HIGHEST PC1 (high crime):\")\n",
    "print(pc1_scores.tail(5))\n",
    "\n",
    "print(\"\\n→ PC1 represents overall crime severity\")\n",
    "print(\"  All crime variables have similar negative loadings\")\n",
    "print(\"  High PC1 = High crime rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PC2 interpretation\n",
    "pc2_scores = scores_df['PC2'].sort_values()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PC2 INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPC2 loadings:\")\n",
    "print(loadings['PC2'].sort_values(ascending=False))\n",
    "\n",
    "print(f\"\\nStates with LOWEST PC2:\")\n",
    "print(pc2_scores.head(5))\n",
    "\n",
    "print(f\"\\nStates with HIGHEST PC2:\")\n",
    "print(pc2_scores.tail(5))\n",
    "\n",
    "print(\"\\n→ PC2 contrasts UrbanPop vs violent crimes\")\n",
    "print(\"  High PC2 = High urbanization, lower violent crime rate\")\n",
    "print(\"  Low PC2 = Low urbanization, higher violent crime rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between original variables and PCs\n",
    "# Correlation = loading × sqrt(eigenvalue)\n",
    "correlations = loadings.copy()\n",
    "for i in range(pca.n_components_):\n",
    "    correlations.iloc[:, i] = loadings.iloc[:, i] * np.sqrt(pca.explained_variance_[i])\n",
    "\n",
    "print(\"Correlations between original variables and PCs:\")\n",
    "print(correlations)\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlations, annot=True, fmt='.3f', cmap='RdBu_r', \n",
    "           center=0, vmin=-1, vmax=1, cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Correlation: Original Variables vs Principal Components', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Choosing Number of Components\n",
    "\n",
    "How many PCs should we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "summary_df = pd.DataFrame({\n",
    "    'PC': [f'PC{i+1}' for i in range(pca.n_components_)],\n",
    "    'Std Dev': std_devs,\n",
    "    'Variance': pca.explained_variance_,\n",
    "    'Prop. Var': pve,\n",
    "    'Cumulative Var': cumulative_pve\n",
    "})\n",
    "\n",
    "print(\"PCA Summary:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DECISION RULES:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"1. Kaiser Rule (eigenvalue > 1): Keep PC1 and PC2\")\n",
    "print(f\"   PC1 variance = {pca.explained_variance_[0]:.2f} > 1 ✓\")\n",
    "print(f\"   PC2 variance = {pca.explained_variance_[1]:.2f} > 1 ✓\")\n",
    "print(f\"   PC3 variance = {pca.explained_variance_[2]:.2f} < 1 ✗\")\n",
    "\n",
    "print(f\"\\n2. Scree Plot Elbow: Around PC2-PC3\")\n",
    "\n",
    "print(f\"\\n3. Variance Threshold (80%): Need {np.argmax(cumulative_pve >= 0.8) + 1} PCs\")\n",
    "\n",
    "print(f\"\\n→ Recommendation: Keep 2 components ({cumulative_pve[1]*100:.1f}% variance)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduced dataset with 2 PCs\n",
    "USArrests_pca = scores_df[['PC1', 'PC2']].copy()\n",
    "\n",
    "print(f\"Original data: {USArrests.shape}\")\n",
    "print(f\"Reduced data: {USArrests_pca.shape}\")\n",
    "print(f\"\\nDimensionality reduced from 4 to 2!\")\n",
    "print(f\"Retained {cumulative_pve[1]*100:.1f}% of variance\")\n",
    "\n",
    "print(\"\\nReduced dataset (first 10 states):\")\n",
    "print(USArrests_pca.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered:\n",
    "\n",
    "### **Principal Component Analysis (PCA)**\n",
    "- **Goal**: Reduce dimensionality while preserving variance\n",
    "- **Method**: Find orthogonal directions of maximum variance\n",
    "- **Output**: New uncorrelated variables (principal components)\n",
    "\n",
    "### **Key Steps**\n",
    "1. **Standardize** data (mean=0, std=1)\n",
    "2. Compute **covariance matrix**\n",
    "3. Find **eigenvectors** (principal components)\n",
    "4. **Project** data onto PCs\n",
    "\n",
    "### **Interpretation**\n",
    "- **Loadings**: How original variables contribute to PCs\n",
    "- **Scores**: Position of observations in PC space\n",
    "- **Variance explained**: How much information each PC captures\n",
    "\n",
    "### **Choosing Number of Components**\n",
    "- **Kaiser Rule**: Keep PCs with eigenvalue > 1\n",
    "- **Scree Plot**: Look for \"elbow\"\n",
    "- **Variance Threshold**: e.g., 80% cumulative variance\n",
    "- **Cross-validation**: For supervised learning\n",
    "\n",
    "### **Biplot**\n",
    "- **Observations** (blue points): States\n",
    "- **Variables** (red arrows): Original features\n",
    "- **Interpretation**: Direction shows correlation\n",
    "\n",
    "### **USArrests Results**\n",
    "- **PC1 (62% variance)**: Overall crime severity\n",
    "  - All crime variables load similarly\n",
    "  - High PC1 = high crime state\n",
    "- **PC2 (25% variance)**: Urbanization vs violent crime\n",
    "  - Positive loading on UrbanPop\n",
    "  - Negative loading on violent crimes\n",
    "- **2 PCs capture 87%** of total variance\n",
    "\n",
    "### **When to Use PCA**\n",
    "- ✅ Exploratory data analysis\n",
    "- ✅ Visualization (reduce to 2-3 dimensions)\n",
    "- ✅ Remove multicollinearity\n",
    "- ✅ Noise reduction\n",
    "- ✅ Speed up algorithms (fewer features)\n",
    "\n",
    "### **Limitations**\n",
    "- ❌ Linear method (won't capture non-linear patterns)\n",
    "- ❌ PCs are linear combinations (hard to interpret)\n",
    "- ❌ Assumes variance = information\n",
    "- ❌ Sensitive to scaling (always standardize!)\n",
    "\n",
    "### **Key Takeaway**\n",
    "PCA transforms correlated variables into uncorrelated principal components,\n",
    "enabling dimensionality reduction while preserving most information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}